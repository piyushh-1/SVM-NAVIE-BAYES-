{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "_EEqJXBz27k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS1 Information Gain (IG) is a metric used in Decision Trees to decide which feature to split on at each node. It measures how much uncertainty (entropy) is reduced after splitting the dataset on a particular feature.\n",
        "\n",
        "1. Entropy (Measure of Impurity)\n",
        "\n",
        "Entropy quantifies how mixed the classes are in a dataset.\n",
        "\n",
        "Entropy(S)=‚àíi=1‚àën‚Äãpi‚Äãlog2‚Äã(pi‚Äã)\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëù\n",
        "ùëñ\n",
        "= proportion of class\n",
        "ùëñ\n",
        "i in dataset\n",
        "ùëÜ\n",
        "S\n",
        "\n",
        "Entropy = 0 ‚Üí pure node (only one class)\n",
        "\n",
        "Entropy = 1 (or higher) ‚Üí more disorder\n",
        "\n",
        "\n",
        "2. Information Gain (Definition)\n",
        "\n",
        "\n",
        "Information Gain(S,A)=Entropy(S)‚àív‚ààValues(A)‚àë‚Äã‚à£S‚à£‚à£Sv‚Äã‚à£‚Äã√óEntropy(Sv‚Äã)\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "S = original dataset\n",
        "\n",
        "\n",
        "A = attribute (feature)\n",
        "\n",
        "\n",
        "S\n",
        "v\n",
        "= subset of data where attribute\n",
        "A has value\n",
        "v\n",
        "\n",
        "\n",
        "3. Intuition Behind Information Gain\n",
        "\n",
        "High Information Gain ‚Üí feature creates pure, well-separated nodes\n",
        "\n",
        "Low Information Gain ‚Üí feature does not help much in classification\n",
        "\n",
        "Decision Trees always choose the feature with maximum Information Gain for splitting\n",
        "\n",
        "4. How It Is Used in Decision Trees\n",
        "\n",
        "1 Calculate entropy of the full dataset.\n",
        "\n",
        "2 For each feature:\n",
        "\n",
        ".Split the dataset by feature values.\n",
        "\n",
        ".Calculate entropy of each subset.\n",
        "\n",
        ".Compute Information Gain.\n",
        "\n",
        "3 Choose the feature with highest Information Gain.\n",
        "\n",
        "4 Repeat recursively for child nodes until:\n",
        "\n",
        ".All samples belong to one class, or\n",
        "\n",
        ".No features remain, or\n",
        "\n",
        ".Stopping criteria are met.\n",
        "\n",
        "5. Example (Simple)\n",
        "\n",
        "If:\n",
        "\n",
        "Entropy before split = 1.0\n",
        "\n",
        "Entropy after split on feature Weather = 0.4\n",
        "\n",
        "IG=1.0‚àí0.4=0.6\n",
        "\n",
        "\n",
        "6. Important Note\n",
        "\n",
        "ID3 algorithm uses Information Gain\n",
        "\n",
        "C4.5 improves it using Gain Ratio (to avoid bias toward attributes with many values)"
      ],
      "metadata": {
        "id": "ZtO2ksuu3Det"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n"
      ],
      "metadata": {
        "id": "aXeJ0G0n4w9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS2 Gini Impurity and Entropy are the two most commonly used measures to quantify node impurity in Decision Trees. Both aim to evaluate how mixed the classes are in a dataset, but they differ in formulation, behavior, and use cases.\n",
        "\n",
        "1. Definition & Formula\n",
        "\n",
        "| Measure           | Formula                                    |\n",
        "| ----------------- | ------------------------------------------ |\n",
        "| **Gini Impurity** | ( \\text{Gini} = 1 - \\sum p_i^2 )           |\n",
        "| **Entropy**       | ( \\text{Entropy} = -\\sum p_i \\log_2(p_i) ) |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äãis the probability of class\n",
        "ùëñ\n",
        "i.\n",
        "\n",
        "\n",
        "2. Interpretation\n",
        "\n",
        "| Aspect        | Gini Impurity                           | Entropy                            |\n",
        "| ------------- | --------------------------------------- | ---------------------------------- |\n",
        "| Meaning       | Probability of incorrect classification | Measure of uncertainty or disorder |\n",
        "| Value = 0     | Pure node                               | Pure node                          |\n",
        "| Maximum value | 0.5 (binary classification)             | 1 (binary classification)          |\n",
        "\n",
        "\n",
        "3. Computational Efficiency\n",
        "\n",
        ".Gini Impurity\n",
        "\n",
        "Faster to compute (no logarithms)\n",
        "\n",
        "Preferred in large datasets\n",
        "\n",
        ".Entropy\n",
        "\n",
        "Computationally expensive (uses log function)\n",
        "\n",
        "Slightly more precise in measuring uncertainty\n",
        "\n",
        "\n",
        "4. Sensitivity to Class Distribution\n",
        "\n",
        "| Feature                    | Gini           | Entropy        |\n",
        "| -------------------------- | -------------- | -------------- |\n",
        "| Sensitivity to node purity | Less sensitive | More sensitive |\n",
        "| Penalizes mixed nodes      | Moderately     | Strongly       |\n",
        "\n",
        "\n",
        "Entropy reacts more sharply to changes near pure nodes, which can lead to slightly different splits.\n",
        "\n",
        "5. Split Quality & Accuracy\n",
        "\n",
        ".In practice, both often produce very similar trees\n",
        "\n",
        ".Entropy may create more balanced trees\n",
        "\n",
        ".Gini may isolate the most frequent class faster\n",
        "\n",
        "6. Algorithm Usage\n",
        "\n",
        "| Algorithm | Measure Used         |\n",
        "| --------- | -------------------- |\n",
        "| CART      | Gini Impurity        |\n",
        "| ID3       | Entropy              |\n",
        "| C4.5      | Entropy / Gain Ratio |\n",
        "\n",
        "\n",
        "7. When to Use Which?\n",
        "\n",
        "| Situation                 | Recommended Measure |\n",
        "| ------------------------- | ------------------- |\n",
        "| Large datasets            | Gini (faster)       |\n",
        "| Theoretical clarity       | Entropy             |\n",
        "| Highly imbalanced classes | Entropy             |\n",
        "| Default in practice       | Gini                |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RDyisfGo43pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 What is Pre-Pruning in Decision Trees?\n"
      ],
      "metadata": {
        "id": "DfAhYaGk51us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS3 Pre-pruning (also called early stopping) in Decision Trees is a technique used to prevent overfitting by stopping the growth of the tree before it perfectly fits the training data.\n",
        "\n",
        "Instead of growing a full tree and trimming it later, pre-pruning decides in advance when to stop splitting.\n",
        "\n",
        "#Why Pre-Pruning Is Needed\n",
        "\n",
        "Decision Trees can:\n",
        "\n",
        ".Learn noise and outliers\n",
        "\n",
        ".Become very deep and complex\n",
        "\n",
        ".Perform poorly on unseen data (overfitting)\n",
        "\n",
        "Pre-pruning controls this by limiting tree growth early.\n",
        "\n",
        "#How Pre-Pruning Works\n",
        "\n",
        "At each node, the algorithm checks stopping conditions. If any condition is met, the node becomes a leaf, even if further splits are possible.\n",
        "\n",
        "# Common Pre-Pruning Criteria\n",
        "\n",
        "1.Maximum Depth\n",
        "\n",
        "Stop if tree reaches a certain depth\n",
        "(e.g., max_depth = 5)\n",
        "\n",
        "2.Minimum Samples per Split\n",
        "\n",
        "Split only if node has at least a minimum number of samples\n",
        "(e.g., min_samples_split = 10)\n",
        "\n",
        "3.Minimum Samples per Leaf\n",
        "\n",
        "Each leaf must contain a minimum number of samples\n",
        "(e.g., min_samples_leaf = 5)\n",
        "\n",
        "4.Minimum Impurity Decrease\n",
        "\n",
        "Split only if impurity reduction exceeds a threshold\n",
        "(e.g., min_impurity_decrease = 0.01)\n",
        "\n",
        "5.Maximum Number of Leaf Nodes\n",
        "\n",
        "Restrict total leaf nodes in the tree\n",
        "\n",
        "#Advantages of Pre-Pruning\n",
        "\n",
        "‚úÖ Reduces overfitting\n",
        "‚úÖ Faster training\n",
        "‚úÖ Simpler, more interpretable trees\n",
        "\n",
        "#Disadvantages of Pre-Pruning\n",
        "\n",
        "‚ùå Risk of underfitting\n",
        "‚ùå May stop useful splits too early\n",
        "‚ùå Requires careful parameter tuning\n",
        "\n",
        "#Pre-Pruning vs Post-Pruning\n",
        "\n",
        "| Aspect       | Pre-Pruning             | Post-Pruning           |\n",
        "| ------------ | ----------------------- | ---------------------- |\n",
        "| When applied | Before tree fully grows | After full tree growth |\n",
        "| Risk         | Underfitting            | More computation       |\n",
        "| Complexity   | Lower                   | Higher                 |\n",
        "| Accuracy     | May be lower            | Often higher           |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcE8uKE057Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "1WCdIq097B5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS4\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create Decision Tree model using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances with feature names\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zci-6pVd7-PV",
        "outputId": "ac596646-38dc-4056-9a08-e3be34d758c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5 What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "SHbAh2k28JCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS5 A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its main goal is to find an optimal decision boundary (hyperplane) that best separates data points of different classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Core Idea of SVM\n",
        "\n",
        "SVM chooses the hyperplane that:\n",
        "\n",
        ".Maximizes the margin (distance) between the hyperplane and the nearest data points of each class\n",
        "\n",
        ".These nearest points are called support vectors\n",
        "\n",
        "A larger margin usually leads to better generalization on unseen data.\n",
        "\n",
        "#Key Concepts\n",
        "1. Hyperplane\n",
        "\n",
        "In 2D: a line\n",
        "\n",
        "In 3D: a plane\n",
        "\n",
        "In higher dimensions: a hyperplane\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "w‚ãÖx+b=0\n",
        "\n",
        "\n",
        "2. Support Vectors\n",
        "\n",
        ".Data points closest to the hyperplane\n",
        "\n",
        ".They define the position and orientation of the decision boundary\n",
        "\n",
        "3. Margin\n",
        "\n",
        ".The distance between the hyperplane and the closest support vectors\n",
        "\n",
        ".SVM maximizes this margin\n",
        "\n",
        "#Types of SVM\n",
        "1. Linear SVM\n",
        "\n",
        ".Used when data is linearly separable\n",
        "\n",
        ".Uses a straight-line hyperplane\n",
        "\n",
        "2. Non-Linear SVM\n",
        "\n",
        ".Uses kernel functions to map data into higher dimensions\n",
        "\n",
        "Common kernels:\n",
        "\n",
        ".Linear\n",
        "\n",
        ".Polynomial\n",
        "\n",
        ".Radial Basis Function (RBF)\n",
        "\n",
        ".Sigmoid\n",
        "\n",
        "#Advantages of SVM\n",
        "\n",
        "‚úÖ Effective in high-dimensional spaces\n",
        "‚úÖ Works well when number of features > number of samples\n",
        "‚úÖ Robust to overfitting (with proper kernel and parameters)\n",
        "\n",
        "#Disadvantages of SVM\n",
        "\n",
        "‚ùå Computationally expensive for large datasets\n",
        "‚ùå Harder to interpret than Decision Trees\n",
        "‚ùå Choice of kernel and parameters is critical\n",
        "\n",
        "#Common Applications\n",
        "\n",
        ".Face recognition\n",
        "\n",
        ".Text classification\n",
        "\n",
        ".Bioinformatics\n",
        "\n",
        ".Image classification\n",
        "\n"
      ],
      "metadata": {
        "id": "eBbwxOGH8PmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6 What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "wtNVb2NY9S0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS6 The Kernel Trick in Support Vector Machines (SVM) is a technique that allows SVMs to handle non-linearly separable data by implicitly mapping data into a higher-dimensional feature space, without explicitly computing that mapping.\n",
        "\n",
        "#Why the Kernel Trick Is Needed\n",
        "\n",
        ".Some datasets cannot be separated by a straight line in the original feature space.\n",
        "\n",
        ".By mapping data to a higher dimension, it may become linearly separable.\n",
        "\n",
        ".Explicitly computing this mapping can be computationally expensive or even infeasible.\n",
        "\n",
        "The kernel trick avoids this cost.\n",
        "\n",
        "\n",
        "#Core Idea\n",
        "\n",
        "Instead of computing:\n",
        "\n",
        "œï(x)\n",
        "\n",
        "(explicit feature transformation)\n",
        "\n",
        "SVM computes:\n",
        "\n",
        "K(xi‚Äã,xj‚Äã)=œï(xi‚Äã)‚ãÖœï(xj‚Äã)\n",
        "\n",
        "Directly using a kernel function that measures similarity between two points.\n",
        "\n",
        "\n",
        "#Common Kernel Functions\n",
        "1. Linear Kernel\n",
        "\n",
        "K(xi‚Äã,xj‚Äã)=xi‚Äã‚ãÖxj‚Äã\n",
        "\n",
        ".No transformation\n",
        "\n",
        ".Used for linearly separable data\n",
        "\n",
        "2. Polynomial Kernel\n",
        "\n",
        "K(xi‚Äã,xj‚Äã)=(xi‚Äã‚ãÖxj‚Äã+c)d\n",
        "\n",
        ".Captures polynomial relationships\n",
        "\n",
        "3. Radial Basis Function (RBF / Gaussian Kernel)\n",
        "\n",
        "K(xi‚Äã,xj‚Äã)=exp(‚àíŒ≥‚à•xi‚Äã‚àíxj‚Äã‚à•2)\n",
        "\n",
        ".Most popular\n",
        "\n",
        ".Handles complex, non-linear boundaries\n",
        "\n",
        "4. Sigmoid Kernel\n",
        "\n",
        "K(xi‚Äã,xj‚Äã)=tanh(Œ±xi‚Äã‚ãÖxj‚Äã+c)\n",
        "\n",
        ".Similar to neural networks\n",
        "\n",
        "#Advantages of the Kernel Trick\n",
        "\n",
        "‚úÖ Enables non-linear classification\n",
        "‚úÖ No need to compute high-dimensional features explicitly\n",
        "‚úÖ Efficient and flexible\n",
        "\n",
        "#Limitations\n",
        "\n",
        "‚ùå Kernel choice is problem-dependent\n",
        "‚ùå Can be slow for very large datasets\n",
        "‚ùå Harder to interpret model behavior\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7hw9RVf9ZDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "vs-TS2jB-wW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train SVM with Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Linear Kernel SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF Kernel SVM Accuracy:\", rbf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qRM6vsr-_PZ",
        "outputId": "f8d3a3ea-c000-4c6d-caa5-ff65512ea309"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel SVM Accuracy: 0.9814814814814815\n",
            "RBF Kernel SVM Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8 What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n"
      ],
      "metadata": {
        "id": "ZvGZHzdz_eEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS8 Na√Øve Bayes is a probabilistic supervised learning classifier based on Bayes‚Äô Theorem. It is widely used for classification tasks, especially in text and spam classification.\n",
        "\n",
        "#What is Na√Øve Bayes?\n",
        "\n",
        "Na√Øve Bayes predicts the class\n",
        "ùê∂\n",
        "C of a data point\n",
        "ùëã\n",
        "X by computing:\n",
        "\n",
        "P(C‚à£X)=P(X)P(X‚à£C)P(C)‚Äã\n",
        "\n",
        "The class with the highest posterior probability is chosen.\n",
        "\n",
        "# Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "\n",
        "It is called na√Øve because it makes a strong simplifying assumption:\n",
        "\n",
        "All features are conditionally independent given the class.\n",
        "\n",
        "This means each feature contributes independently to the final decision, which is often not true in real-world data.\n",
        "\n",
        "# Example of the Na√Øve Assumption\n",
        "\n",
        "If features are:\n",
        "\n",
        ".Fever\n",
        "\n",
        ".Cough\n",
        "\n",
        "Na√Øve Bayes assumes:\n",
        "\n",
        ".Fever and cough are independent, given the disease\n",
        "\n",
        "In reality, they are usually correlated, hence the assumption is na√Øve.\n",
        "\n",
        "# Types of Na√Øve Bayes Classifiers\n",
        "\n",
        "1.Gaussian Na√Øve Bayes\n",
        "\n",
        ".For continuous data\n",
        "\n",
        ".Assumes normal distribution\n",
        "\n",
        "2.Multinomial Na√Øve Bayes\n",
        "\n",
        ".For discrete counts (e.g., word frequencies)\n",
        "\n",
        "3.Bernoulli Na√Øve Bayes\n",
        "\n",
        ".For binary features (0/1)\n",
        "\n",
        "#Advantages\n",
        "\n",
        "‚úÖ Simple and fast\n",
        "‚úÖ Works well with high-dimensional data\n",
        "‚úÖ Effective even with small datasets\n",
        "\n",
        "#Limitations\n",
        "\n",
        "‚ùå Independence assumption often unrealistic\n",
        "‚ùå Performs poorly when features are highly correlated\n",
        "\n"
      ],
      "metadata": {
        "id": "WhSaeLsb_m1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9 Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes"
      ],
      "metadata": {
        "id": "4_1w68Y0ApVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS9 Na√Øve Bayes classifiers differ mainly in the type of data they assume and the probability distribution used to model features.\n",
        "\n",
        "\n",
        "#1. Gaussian Na√Øve Bayes (GNB)\n",
        "# Assumption\n",
        "\n",
        "Features are continuous and follow a normal (Gaussian) distribution.\n",
        "\n",
        "# Probability Model\n",
        "\n",
        "P(x‚à£C)=2œÄœÉC2‚Äã\n",
        "‚Äã1‚Äãexp(‚àí2œÉC2‚Äã(x‚àíŒºC‚Äã)2‚Äã)\n",
        "\n",
        "#Used When\n",
        "\n",
        ".Data contains real-valued measurements.\n",
        "\n",
        "#Examples\n",
        "\n",
        ".Height, weight, temperature\n",
        "\n",
        ".Medical measurements\n",
        "\n",
        "#Pros / Cons\n",
        "\n",
        ".‚úÖ Works well for continuous data\n",
        "\n",
        ".‚ùå Assumption of normality may not always hold\n",
        "\n",
        "#2. Multinomial Na√Øve Bayes (MNB)\n",
        "#Assumption\n",
        "\n",
        ".Features are discrete counts.\n",
        "\n",
        ".Data follows a multinomial distribution.\n",
        "\n",
        "#Probability Model\n",
        "\n",
        ".Uses feature frequencies.\n",
        "\n",
        "#Used When\n",
        "\n",
        ".Text classification with word counts or TF-IDF.\n",
        "\n",
        "#Examples\n",
        "\n",
        ".Spam detection\n",
        "\n",
        ".Sentiment analysis\n",
        "\n",
        ".Document classification\n",
        "\n",
        "#Pros / Cons\n",
        "\n",
        ".‚úÖ Excellent for text data\n",
        "\n",
        ".‚ùå Not suitable for continuous values\n",
        "\n",
        "\n",
        "\n",
        "#3. Bernoulli Na√Øve Bayes (BNB)\n",
        "#Assumption\n",
        "\n",
        ".Features are binary (0 or 1).\n",
        "\n",
        ".Data follows a Bernoulli distribution.\n",
        "\n",
        "#Probability Model\n",
        "\n",
        ".Models presence or absence of features.\n",
        "\n",
        "#Used When\n",
        "\n",
        ".Binary feature vectors.\n",
        "\n",
        "#Examples\n",
        "\n",
        ".Word present or not present in a document\n",
        "\n",
        ".Yes/No attributes\n",
        "\n",
        "#Pros / Cons\n",
        "\n",
        ".‚úÖ Simple and effective for binary data\n",
        "\n",
        ".‚ùå Ignores frequency information\n",
        "\n",
        "#Key Differences at a Glance\n",
        "\n",
        "| Aspect                | Gaussian NB  | Multinomial NB       | Bernoulli NB    |\n",
        "| --------------------- | ------------ | -------------------- | --------------- |\n",
        "| Feature type          | Continuous   | Discrete counts      | Binary          |\n",
        "| Distribution          | Gaussian     | Multinomial          | Bernoulli       |\n",
        "| Best for              | Numeric data | Text (counts/TF-IDF) | Binary features |\n",
        "| Uses frequency        | ‚ùå            | ‚úÖ                    | ‚ùå               |\n",
        "| Uses presence/absence | ‚ùå            | ‚ùå                    | ‚úÖ               |\n"
      ],
      "metadata": {
        "id": "2f9KQt1ZAznw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10 Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "c8Af7OfMCHXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ANS10\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gaussian Na√Øve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Na√Øve Bayes Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7CoZQ27CfG_",
        "outputId": "18aed4e8-3eea-49aa-935e-c10ee2000e5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Na√Øve Bayes Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}